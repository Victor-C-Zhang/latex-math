\documentclass{article}
\usepackage[utf8]{inputenc}

\title{481 - Homework 5}
\author{Victor Zhang}
\date{October 4, 2020}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{natbib}
\usepackage{graphicx}
% \usepackage{changepage}
\usepackage{amssymb}
\usepackage{xfrac}
% \usepackage{bm}
% \usepackage{empheq}

\newcommand{\contra}{\raisebox{\depth}{\#}}
\newcommand{\var}[1]{\mathrm{var}(#1)}
\newcommand{\inv}[1]{\left( #1 \right)^{-1}}

\newenvironment{myindentpar}[1]
  {\begin{list}{}
          {\setlength{\leftmargin}{#1}}
          \item[]
  }
  {\end{list}}

\pagestyle{empty}

\begin{document}

\maketitle
% \begin{center}
% {\huge Econ 482 \hspace{0.5cm} HW 3}\
% {\Large \textbf{Victor Zhang}}\
% {\Large February 18, 2020}
% \end{center}

\section*{10.13 (Copied from homework 4)}
$$E[(\hat{\Theta}-\theta)^2] = \mathrm{var}(\hat{\Theta})$$
$$E[\hat{\Theta}^2] - \theta^2 = \mathrm{var}(\hat{\Theta})$$
$$E[\hat{\Theta}^2] = \theta^2 + \mathrm{var}(\hat{\Theta}) \neq \theta^2 \; \Box$$

\section*{10.14}
By definition the binomial with parameters $n,\theta$ is the sum of $n$ independent Bernoulli variables with parameter $\theta$. Thus, $X/n = \overline{X}$, the sample mean of $n$ such i.i.d. Bernoulli variables. Denote $X \sim Ber(\theta)$ and note $E[X^2] = \theta$. $\overline{X}$ is clearly unbiased, so we may appeal to Cram\'er-Rao,
\begin{equation*}
\begin{split}
  \var{\hat{\theta}} &\geq \inv{n E\left[\left(\frac{\partial}{\partial\theta} \ln f(X)\right)^2 \right]}\\
  &= \inv{nE\left[\left(\frac{\partial}{\partial\theta} x\ln\theta + (1-x)\ln(1-\theta)\right)^2 \right]}\\
  &= \inv{nE\left[\left(\frac{X}{\theta} - \frac{1-X}{1-\theta} \right)^2 \right]}\\
  &= \inv{n\left( \frac{1}{\theta} + \frac{1}{1-\theta} \right)}\\
  &= \frac{\theta(1-\theta)}{n}
\end{split}
\end{equation*}
This is exactly $\var{\overline{X}}$ so we are done $\Box$

\section*{10.15}
Suppose $X \sim Pois(\lambda)$. $E[X^2] = E[X]^2 + \var{X} = \lambda^2 + \lambda$. The sample mean is unbiased, so by Cram\'er-Rao,
\begin{equation*}
\begin{split}
  \var{\hat{\lambda}} &\geq \inv{n E\left[\left(\frac{\partial}{\partial\lambda} \ln f(X)\right)^2 \right]}\\
  &= \inv{nE\left[\left(\frac{\partial}{\partial\lambda} x\ln\lambda -\lambda - \ln x!\right)^2 \right]}\\
  &= \inv{nE\left[\left(\frac{X - \lambda}{\lambda} \right)^2 \right]}\\
  &= \inv{n\left( \frac{1}{\lambda} \right)}\\
  &= \frac{\lambda}{n}
\end{split}
\end{equation*}
This is exactly the sample mean variance, so we are done $\Box$

\section*{10.16}
Clearly, $a_1 + a_2 = 1$. Since the two estimators are independent, $\var{a_1\hat{\Theta}_1 + a_2\hat{\Theta}_2} = (3a_1^2 + a_2^2)\var{\hat{\Theta}}$. Using some algebra and calculus, it can be shown that this value is minimized when $a_1 = \frac{1}{4}, a_2 = \frac{3}{4}$ $\Box$

\section*{10.17}
Suppose $X \sim Exp(\lambda)$. $E[X^2] = E[X]^2 + \var{X} = 2\lambda^2$. The sample mean is unbiased, so by Cram\'er-Rao,
\begin{equation*}
\begin{split}
  \var{\hat{\lambda}} &\geq \inv{n E\left[\left(\frac{\partial}{\partial\lambda} \ln f(X)\right)^2 \right]}\\
  &= \inv{nE\left[\left(\frac{\partial}{\partial\lambda} -\ln\lambda -\frac{x}{\lambda}\right)^2 \right]}\\
  &= \inv{nE\left[\left(\frac{X}{\lambda^2} - \frac{1}{\lambda} \right)^2 \right]}\\
  &= \inv{n\left( \frac{1}{\lambda^2} \right)}\\
  &= \frac{\lambda^2}{n}
\end{split}
\end{equation*}
This is exactly the sample mean variance, so we are done $\Box$

\section*{10.21}
\subsection*{10.21.a}
$$E[\omega \overline{X}_1 + (1-\omega)\overline{X}_2] = \omega \mu + (1-\omega)\mu = \mu \; \Box$$
\subsection*{10.21.b}
\begin{equation}
\begin{split}
  \var{\omega \overline{X}_1 + (1-\omega)\overline{X}_2} &= \omega^2 \var{\overline{X}_1} + (1-\omega)^2\var{\overline{X}_2}\\
  &= \omega^2 \sigma_1^2 + (1-\omega)^2\sigma_2^2\\
  &= \omega^2(\sigma_1^2 + \sigma_2^2) - 2\omega\sigma_2^2 + \sigma_2^2
\end{split}
\end{equation}
This value is minimized when the derivative w.r.t $\omega$ is zero, in other words when $\omega = \frac{\sigma_2^2}{\sigma_1^2 + \sigma_2^2}$ $\Box$

\section*{10.22}
Using (1) the variance when $\omega = \frac{1}{2}$ is $\frac{\sigma_1^2 + \sigma_2^2}{4}$. The variance using $\omega$ as defined in 10.21 is $\frac{\sigma_1^2 \sigma_2^2}{\sigma_1^2 + \sigma_2^2}$. The relative efficiency of $\omega = \frac{1}{2}$ to the $\omega$ defined in 10.21 is $\frac{4 \sigma_1^2 \sigma_2^2}{\left(\sigma_1^2 + \sigma_2^2\right)^2}$. Note by Cauchy-Schwarz this is trivially less than 1 $\Box$

\section*{10.23}
\begin{equation}
\var{\omega \overline{X}_1 + (1-\omega)\overline{X}_2} = \sigma^2 \left(\frac{\omega^2}{n_1} + \frac{(1-\omega)^2}{n_2} \right)
\end{equation}

This value is minimized when the derivative w.r.t $\omega$ is zero, in other words when $\omega = \frac{n_1}{n_1 + n_2}$ $\Box$ 

\section*{10.24}
Using (1) the variance when $\omega = \frac{1}{2}$ is $\sigma^2\frac{n_1+n_2}{4n_2n_2}$. The variance using $\omega$ as defined in 10.21 is $\sigma^2\frac{1}{n_1+n_2}$. The relative efficiency of $\omega = \frac{1}{2}$ to the $\omega$ defined in 10.21 is $\frac{4 n_1n_2}{\left(n_1^2 + n_2^2\right)^2}$. This is also trivially less than 1 $\Box$

\section*{10.25}
$$\var{\frac{X_1+2X_2+X_3}{4}} = \frac{\sigma^2}{16} + \frac{\sigma^2}{4} + \frac{\sigma^2}{16} = \frac{3}{8}\sigma^2$$
The variance of the sample mean is $\frac{1}{3}\sigma^2$, so the relative efficiency of the former estimator to the sample mean is $\frac{8}{9}$ $\Box$

\section*{10.26}
Note $Y_1 \sim Exp(\frac{\lambda}{2})$. $\var{Y_1} = \frac{\lambda^2}{4}$ so $\var{2Y_1} = \lambda^2$. $\var{\overline{X}} = \frac{\lambda^2}{2}$ so the relative efficiency of $Y_1$ to $\overline{X}$ is $\frac{1}{2}$ $\Box$

\section*{10.50}
Denote by $X^s$ the sampling distribution. By method of moments, we estimate $\mu = E[X] = E[X^s] = \overline{X}$ and $\sigma^2 = \var{X} = \var{X^s} = (s')^2$ $\Box$

\section*{10.51}
Denote by $X^s$ the sampling distribution. By method of moments, we estimate $\lambda = E[X] = E[X^s] = \overline{X}$ $\Box$

\section*{10.52}
Denote by $X^s$ the sampling distribution. By method of moments, we estimate $\frac{\beta}{2} = E[X] = E[X^s] = \overline{X}$. Thus, $\hat{\beta} = 2\overline{X}$ $\Box$

\section*{10.53}
Denote by $X^s$ the sampling distribution. By method of moments, we estimate $\lambda = E[X] = E[X^s] = \overline{X}$ $\Box$

\section*{10.54}
Denote by $X^s$ the sampling distribution. By method of moments, we estimate $\frac{\alpha}{\alpha + 1} = E[X] = E[X^s] = \overline{X}$. Thus, $\hat{\alpha} = \frac{\overline{X}}{1-\overline{X}}$ $\Box$

\section*{10.55}


\end{document}

% List of tex snippets:
%   - tex-header (this)
%   - R      --> \mathbb{R}
%   - Z      --> \mathbb{Z}
%   - B      --> \mathcal{B}
%   - E      --> \mathcal{E}
%   - M      --> \mathcal{M}
%   - m      --> \mathfrak{m}({#1})
%   - normlp --> \norm{{#1}}_{L^{{#2}}}
